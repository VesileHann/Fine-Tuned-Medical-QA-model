{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3dd9c7a7",
   "metadata": {},
   "source": [
    "# Fine-Tuned Medical QA Model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba91a88",
   "metadata": {},
   "source": [
    "## Importing Dependencies and Loading Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e5d5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/pythia-410m\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"EleutherAI/pythia-410m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2fd953cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.pad_token=tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b72a4c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess dataset \n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"Malikeh1375/medical-question-answering-datasets\", \"all-processed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebbfa268",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "        num_rows: 246678\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08bb8f38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'input', 'output'],\n",
       "        num_rows: 246678\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset=dataset.remove_columns('__index_level_0__')\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97479eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset into train and test\n",
    "split_dataset=dataset['train'].train_test_split(test_size=0.2,shuffle=True,seed=42)\n",
    "train_dataset=split_dataset['train']\n",
    "test_dataset=split_dataset['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e798e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(197342, 3)\n",
      "(49336, 3)\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset.shape)\n",
    "print(test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76f03e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_func(examples):\n",
    "    # Create structured prompts\n",
    "    prompts = [\n",
    "        f\"{inst}\\n\\nPatient: {inp}\\n\\nDoctor:\"\n",
    "        for inst, inp in zip(examples['instruction'], examples['input'])\n",
    "    ]\n",
    "    \n",
    "    # Combine with responses\n",
    "    full_texts = [\n",
    "        p + resp + tokenizer.eos_token\n",
    "        for p, resp in zip(prompts, examples['output'])\n",
    "    ]\n",
    "\n",
    "    # Tokenize with padding/truncation\n",
    "    tokenized = tokenizer(\n",
    "        full_texts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # Create labels (mask patient input and instruction)\n",
    "    tokenized_prompts = tokenizer(\n",
    "        prompts,\n",
    "        max_length=512,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Mask everything before the doctor's response\n",
    "    labels = []\n",
    "    for i in range(len(tokenized['input_ids'])):\n",
    "        prompt_len = sum(tokenized_prompts['attention_mask'][i]).item()\n",
    "        label = [-100]*prompt_len + tokenized['input_ids'][i][prompt_len:].tolist()\n",
    "        labels.append(label)\n",
    "    \n",
    "    tokenized['labels'] = labels\n",
    "    return tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3f87a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f522b9ede449ea871507509888b5dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/49336 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Apply tokenization to datasets  \n",
    "tokenized_train = train_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    remove_columns=train_dataset.column_names  # Use the split's columns\n",
    ")\n",
    "tokenized_test = test_dataset.map(\n",
    "    tokenize_func,\n",
    "    batched=True,\n",
    "    batch_size=256,\n",
    "    remove_columns=test_dataset.column_names  # Use the split's columns\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a102e0ec",
   "metadata": {},
   "source": [
    "## Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d10226fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer,TrainingArguments\n",
    "training_args=TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.1,\n",
    "    eval_strategy='epoch'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbd91c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train.select(range(2000)),\n",
    "    eval_dataset=tokenized_test.select(range(1000))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e3af4b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [500/500 2:15:01, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.643600</td>\n",
       "      <td>0.589307</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=500, training_loss=0.6435782470703125, metrics={'train_runtime': 8121.3811, 'train_samples_per_second': 0.246, 'train_steps_per_second': 0.062, 'total_flos': 2173886791680000.0, 'train_loss': 0.6435782470703125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "efc705aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [250/250 17:09]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.5893072485923767,\n",
       " 'eval_runtime': 1034.2397,\n",
       " 'eval_samples_per_second': 0.967,\n",
       " 'eval_steps_per_second': 0.242,\n",
       " 'epoch': 1.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0783a044",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_ElethuerAI_medical_model/tokenizer_config.json',\n",
       " './fine_tuned_ElethuerAI_medical_model/special_tokens_map.json',\n",
       " './fine_tuned_ElethuerAI_medical_model/tokenizer.json')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./fine_tuned_ElethuerAI_medical_model\")\n",
    "tokenizer.save_pretrained(\"./fine_tuned_ElethuerAI_medical_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70318d88",
   "metadata": {},
   "source": [
    "### Example usage for generating responses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c5baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated: This may be related to a rupture of the gallbladder.  The pain may be related to the rupture of the gallbladder.  This may be related to a gallbladder stone.  This may be related to a gallbladder infection.  This may be related to an intestines infection.  This may be related to a gallbladder infection.  This may be related to a gallbladder stone.  This may be related to a gallbladder infection.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "Example usage for generating responses:\n",
    "prompt = \"\"\"Act as a doctor.Keep annotations concise. \n",
    "Question: \"Iâ€™ve had sharp pain in my upper abdomen after eating.\"\n",
    "Answer:\"\"\"\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(\n",
    "    inputs.input_ids,\n",
    "    max_new_tokens=200,\n",
    "    temperature=0.7,\n",
    "    top_p=0.9,\n",
    "    do_sample=True,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "\n",
    "response = tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "print(\"Generated:\", response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
